__kernel void two_unused_params(int n, int w, int z, __global int *data, __global int *data1, int x, int y)
Not resolved yet. test_instruction
test_instruction
uint* V0 = (uint*)(DS + ((v4 + 256)&~3)) // ds_add_u32
*V0 = *V0 + v5  // atomic operation
ulong tmp0 // ds_bpermute_b32
for (short i = 0; i < 64; i++)
{
    uint lane_id = v0[(i + (0x10 >> 2)) & 63]
    tmp0[i] = exec & (1ULL << lane_id) != 0) ? v1[lane_id] : 0
}
for (short i = 0; i < 64; i++)
    if (exec & (1ULL << i) != 0)
        v0[i] = tmp0[i]
v3 = *(uint*)(DS + ((v4 + 256)&~3)) // ds_read_b32
v3 = *(ulong*)(DS + ((v4 + 256)&~7)) // ds_read_b64
ulong* V00 = (ulong*)(ds + (v[76:77] + 16 * 8) & ~7) // ds_read2_b64
ulong* V10 = (ulong*)(ds + (v[76:77] + 32 * 8) & ~7)
v90 = *V00 | (ulonglong(*V10) << 64)
uint* V1 // ds_write_b32
V1 = (uint*)(ds + ((v4 + 256) & ~3))
*V1 = v5
ulong* V2 // ds_write_b64
V2 = (ulong*)(ds + ((v4 + 256) & ~7))
*V2 = v[5:6]
ulong* V01 = (ulong*)(ds + (v4 + 256 * 8) & ~7) // ds_write2_b64
ulong* V11 = (ulong*)(ds + (v4 + offset * 8) & ~7)
*V01 = v[5:6]
*V11 = v[7:8]
uint* vm0 = (uint*)(v[5:6] + ) // flat_atomic_add
uint p0 = *vm0; *vm0 = *vm0 + v7; v4 = (glc) ? p0 : v4  // atomic
v2 = *(uint*)(v[2:3] + 0) // flat_load_dword
short* vm1 = (v[2:3] + ) // flat_load_dwordx4
v[1:2][0] = *(uint*)vm1
v[1:2][1] = *(uint*)(vm1 + 4)
v[1:2][2] = *(uint*)(vm1 + 8)
v[1:2][3] = *(uint*)(vm1 + 12)
*(uint32*)(v[0:1] + 0) = v2 // flat_store_dword
*(ulong*)(v[0:1] + 0 = v[2:3] // flat_store_dwordx2
short* vm2 = (v[0:1] + 0) // flat_store_dwordx4
*(uint*)(vm2) = v[2:5][0]
*(uint*)(vm2 + 4) = v[2:5][1]
*(uint*)(vm2 + 8) = v[2:5][2]
*(uint*)(vm2 + 12) = v[2:5][3]
v2 = *(uint*)(v[0:1] + 0 + 0) // global_load_dword
v[2:3] = *(ulong*)(v[0:1] + 0 + 0)  // global_load_dwordx2
*(uint*)(v[2:3] + 0 + 0) = v4 // global_store_dword
*(ulong*)(v[2:3] + 0 + 0) = v[4:5] // global_store_dwordx2
s1 = *(uint*)(smem + (0x30 & ~3)) // s_load_dword
s[0:1] = *(ulong*)(smem + (0x0 & ~3)) // s_load_dwordx2
for (BYTE i = 0; i < 4; i++) // s_load_dword4
    s[0:3][i] = *(uint*)(SMEM + i*4 + (0x4 & ~3))
for (BYTE i = 0; i < 8; i++) // s_load_dword8
    s[0:7][i] = *(uint*)(SMEM + i*4 + (0x30 & ~3))
s[0:1] = exec // s_and_saveexec_b64
exec = vcc & exec
scc = exec != 0
s[0:1] = pc + 4 // s_getpc_b64
s0 = s6 // s_mov_b32
exec = s[4:5] // s_mov_b64
s[0:1] = ~s[2:3] // s_not_b64
scc = s[0:1] != 0
pc = s[0:1] // s_setpc_b64
s[0:1] = pc + 4 // s_swappc_b64
pc = s[2:3]
ulong temp0 = (ulong)s4 + (ulong)s0 // s_add_u32
s0 = temp0
scc = temp0 >> 32
ulong temp1 = (ulong)s13 + (ulong)-1 + scc // s_addc_u32
s9 = temp1
scc = temp1 >> 32
s5 = s0 & 0xffff // s_and_b32
scc = s5 != 0
s[10:11] = s[8:9] & s[10:11] // s_and_b64
scc = s[10:11] != 0
exec = s[4:5] & ~exec // s_andn2_b64
scc = exec != 0
s0 = (int)s1 >> (31 & 31) // s_ashr_i32
scc = s0!=0
uchar shift0 = 0x100010 & 31 // s_bfe_u32
uchar length0 = (0x100010>>16) & 07xf
if (length0==0)
    s0 = 0
if (shift0 + length0 < 32)
    s0 = s0 << (32 - shift0 - length0) >> (32 - length0)
else
    s0 = s0 >> shift0
scc = s0 != 0
uchar shift1 = 0x100010 & 31 // s_bfe_i32
uchar length1 = (0x100010>>16) & 07xf
if (length1==0)
    s0 = 0
if (shift1 + length1 < 32)
    s0 = (int)s0 << (32 - shift1 - length1) >> (32 - length1)
else
    s0 = (int)s0 >> shift1
scc = s0 != 0
vcc = scc ? exec : 0 // s_cselect_b64
s6 = s6 << (6 & 31) // s_lshl_b32
scc = s6!= 0
s[0:1] = s[4:5] << (2 & 63) // s_lshl_b64
scc = s[0:1]!= 0
s0 = s3 >> (2 & 31) // s_lshr_b32
scc = s0!= 0
s0 = min((int)s1, (int)s0) // s_min_i32
scc = (int)s1 < (int)s0
s5 = s5 * s8 // s_mul_i32
s6 = 4 - s9 // s_sub_i32
long temp2 = (long)4 - (long)s9
scc = temp2 > ((1LL << 31) - 1) || temp2 < (-1LL << 31)
ulong temp3 = (ulong)4 - (ulong)s9 // s_sub_u32
s6 = temp3
scc = (temp3>>32)!=0
scc = s0==s1 // s_cmp_eq_i32
scc = s0==s1 // s_cmp_eq_u32
scc = 1==s[0:1] // s_cmp_eq_u64
scc = (int)s0 >= (int)s1 // s_cmp_ge_i32
scc = s10 >= s0 // s_cmp_ge_u32
scc = (int)s2 < (int)s3 // s_cmp_lt_i32
mode = (mode & ~(1U << 27)) | (1U << 27) // s_set_gpr_idx_on
m0 = (m0 & 0xffff0f00) | ((s1 & 15) << 12) | (s0 & 0xff)
scc = 0x0 // s_movk_i32
s2 = s2 * 6 // s_mulk_i32
uint mask0 = (1U << s3) - 1U) << s2 // s_setreg_b32
s0 = (s0& ~mask0) | ((s4 << s2) & mask0)
s_barrier
pc = .L172_0 // s_branch
goto .L172_0
pc = exec==0 ?.L172_0 : pc+4 // s_cbranch_execz
pc = scc==0 ?.L172_0 : pc+4 // s_cbranch_scc0
pc = scc==1 ?.L172_0 : pc+4 // s_cbranch_scc1
pc = vcc != 0 ? .L172_0 : pc + 4 // s_cbranch_vccnz
pc = vcc==0 ?.L172_0 : pc+4 // s_cbranch_vccz
s_nop
s_set_gpr_id_offs_set_gpr_id_off
s_waitcnt                         lgkmcnt(0)
v0 = (float)s0 // v_cvt_f32_u32
v[0:1] = (double)(int)s0 // v_cvt_f64_i32
v0 = 0 // v_cvt_u32_f32
if (!isnan(as_float(s0)))
    v0 = (int)min(convert_int_rtz(as_float(s0)), 4294967295.0)
v2 = s3 // v_mov_b32
uint temp4 = (ulong)s0 + (ulong)v0 // v_add_u32
v0 = CLAMP ? min(temp4, 0xffffffff) : temp4
vcc = 0
ulong mask1 = (1ULL<<LANEID)
vcc = (vcc&~mask1) | ((temp4 >> 32) ? mask1 : 0)
ulong mask2 = (1ULL<<LANEID) // v_addc_u32
uchar cc0 = ((vcc&mask2 ? 1 : 0)
uint temp5 = (ulong)v2 + (ulong)v1 + cc0
vcc = 0
v1 = CLAMP ? min(temp5, 0xffffffff) : temp5
vcc = (vcc&~mask2) | ((temp5 >> 32) ? mask2 : 0)
v2 = 1 & v4 // v_and_b32
v1 = (int)v2 >> (31&31) // v_ashrrev_i32
v[0:1] = (long)v[0:1] >> (30&63) // v_ashrrev_i64
v4 = s[6:7]&(1ULL<<LANEID) ? v1 : v0 // v_cndmask_b32
v4 = v1 << (v0&31) // v_lshlrev_b32
v[0:1] = v[0:1] << (2&63) // v_lshlrev_b64
v[0:1] = v[0:1] >> (2 & 63) // v_lshrrev_b64
v3 = as_float(v2) * as_float(v5) + as_float(v3) // v_mac_f32
v0 = min(s0, s1) // v_min_u32
v3 = as_float(v4) * as_float(v3) // v_mul_f32
int V02 (int)((v0&0x7fffff) | (v0&0x800000 ? 0xff800000 : 0)) // v_mul_i32_i24
int V12 (int)((v0&0x7fffff) | (v0&0x800000 ? 0xff800000 : 0))
v0 = V02 * V12
ulong temp6 = (ulong)0 - (ulong)v3 // v_sub_u32
v6 = CLAMP ? (temp6>>32 ? 0 : temp6) : temp6
vcc = 0
ulong mask3 = (1ULL<<LANEID)
vcc = (vcc&~mask3) | ((temp6>>32) ? mask3 : 0)
v3 = as_float(1.0) - as_float(v2) // v_sub_f32
ulong temp7 = (ulong)v1 - (ulong)s1 // v_subrev_u32
v4 = CLAMP ? (temp7>>32 ? 0 : temp7) : temp7
vcc = 0
ulong mask4 = (1ULL<<LANEID)
vcc = (vcc&~mask4) | ((temp7>>32) ? mask4 : 0)
v0 = s0 ^ s1 // v_xor_b32
v0 = (((ulong)s0) << 32) | s1) >> (s2 & 31) // v_alignbit_b32
v0 = (((ulong)s0) << 32) | s1) >> ((s2 & 3) * 8) // v_alignbyte_b32
v0 = (s0 & s1) | s2 // v_and_or_b32
v0 = (s0 & s1) | (~s0 & s2) // v_bfi_b32
double sf00 = as_double(s0) // v_div_fixup_f64
double sf10 = as_double(s1)
double sf20 = as_double(s2)
if (isnan(sf10) && !isnan(sf20))
    v0 = nan(sf10)
else if (abs(sf20))
    v0 = nan(sf20)
else if (sf10 == 0.0 && sf20 == 0.0)
    v0 = NAN
else if (abs(sf10) == INFINITY)
    v0 = -NAN
else if (sf10 == 0.0)
    v0 = INFINITY * sign(sf10) * sign(sf20)
else if (abs(sf10) == INFINITY)
    v0 = sign(sf10) * sign(sf20) >= 0 ? 0.0 : -0.0
else if (isnan(sf00))
    v0 = sign(sf10) * sign(sf20) * INFINITY
else
    v0 = sf00
v0 = as_double(s0) * as_double(s1) + as_double(s2) // v_fma_f32
v[0:1] = as_double(s[0:1]) * as_double(s[2:3]) // v_mul_f64
v0 = s0 * s2 // v_mul_lo_u32
vcc(LANEID) = (int)1 == (int)v0 // v_cmp_eq_i32
vcc(LANEID) = (uint)1 == (uint)v0 // v_cmp_eq_u32
vcc(LANEID) = as_float(s2) == as_float(v2) // v_cmp_eq_f32
vcc = (uint)s2 >= (uint)v2 // v_cmp_ge_u32
vcc = (ulong)s0 > (uint)v2 // v_cmp_gt_u64
vcc = (int)s0 > (int)v2 // v_cmp_gt_i32
s[6:7] = (int)v0 != (int)1 // v_cmp_lg_i32
s[6:7] = (uint)v0 != (uint)1 // v_cmp_lg_u32
s[6:7] = (uint)v0 < (uint)1 // v_cmp_lt_u32
v_cmpx_class_f64                  vcc, s[0:1], s[2:3]
vcc = as_double(v[2:3]) == as_double(v[4:5]) // v_cmpx_eq_f64
exec = vcc
vcc = (uint)v0 <= (uint)v1 // v_cmpx_le_u32
exec =  = vcc
